{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import batch_norm, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mediwhale/.conda/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#init weight and placeholder\n",
    "\n",
    "num_classes = 1\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, height, width,num_classes],name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, height, width],name=\"y\")\n",
    "\n",
    "p_keep_conv = tf.placeholder(\"float\",name=\"p_keep_conv\")\n",
    "is_training = tf.placeholder(tf.bool,name=\"is_training\")\n",
    "\n",
    "ksize = 5\n",
    "fsize = 64\n",
    "initstdev = 0.01\n",
    "initfun = tf.random_normal_initializer(mean=0.0, stddev=initstdev)\n",
    "\n",
    "weights = {\n",
    "    'conv1_1': tf.get_variable(\"conv1_1\", shape = [3, 3, num_classes, 64], initializer = initfun) ,\n",
    "    'conv1_2': tf.get_variable(\"conv1_2\", shape = [3, 3, 64, 64], initializer = initfun) ,\n",
    "    'conv1_3': tf.get_variable(\"conv1_3\", shape = [3, 3, 128, 64], initializer = initfun) ,\n",
    "    'conv1_4': tf.get_variable(\"conv1_4\", shape = [3, 3, 64, 64], initializer = initfun) ,\n",
    "    \n",
    "    'conv2_1': tf.get_variable(\"conv2_1\", shape = [3, 3, 64, 128], initializer = initfun) ,\n",
    "    'conv2_2': tf.get_variable(\"conv2_2\", shape = [3, 3, 128, 128], initializer = initfun) ,\n",
    "    'conv2_3': tf.get_variable(\"conv2_3\", shape = [3, 3, 256, 128], initializer = initfun) ,\n",
    "    'conv2_4': tf.get_variable(\"conv2_4\", shape = [3, 3, 128, 128], initializer = initfun) ,\n",
    "    'conv2_de': tf.get_variable(\"conv2_de\", shape = [3, 3, 64, 128], initializer = initfun) ,\n",
    "    \n",
    "    'conv3_1': tf.get_variable(\"conv3_1\", shape = [3, 3, 128, 256], initializer = initfun) ,\n",
    "    'conv3_2': tf.get_variable(\"conv3_2\", shape = [3, 3, 256, 256], initializer = initfun) ,\n",
    "    'conv3_3': tf.get_variable(\"conv3_3\", shape = [3, 3, 512, 256], initializer = initfun) ,\n",
    "    'conv3_4': tf.get_variable(\"conv3_4\", shape = [3, 3, 256, 256], initializer = initfun) ,\n",
    "    'conv3_de': tf.get_variable(\"conv3_de\", shape = [3, 3, 128, 256], initializer = initfun) ,\n",
    "    \n",
    "    'conv4_1': tf.get_variable(\"conv4_1\", shape = [3, 3, 256, 512], initializer = initfun) ,\n",
    "    'conv4_2': tf.get_variable(\"conv4_2\", shape = [3, 3, 512, 512], initializer = initfun) ,\n",
    "    'conv4_3': tf.get_variable(\"conv4_3\", shape = [3, 3, 1024, 512], initializer = initfun) ,\n",
    "    'conv4_4': tf.get_variable(\"conv4_4\", shape = [3, 3, 512, 512], initializer = initfun) ,\n",
    "    'conv4_de': tf.get_variable(\"conv4_de\", shape = [3, 3, 256, 512], initializer = initfun) ,\n",
    "    \n",
    "    'conv5_1': tf.get_variable(\"conv5_1\", shape = [3, 3, 512, 1024], initializer = initfun) ,\n",
    "    'conv5_2': tf.get_variable(\"conv5_2\", shape = [3, 3, 1024, 1024], initializer = initfun) ,\n",
    "    'conv5_de': tf.get_variable(\"conv5_de\", shape = [3, 3, 512, 1024], initializer = initfun) ,\n",
    "    \n",
    "    'dense_inner_prod': tf.get_variable(\"dense_inner_prod\", shape= [1, 1, 64, num_classes]\n",
    "                                       , initializer = initfun)\n",
    "}\n",
    "biases = {\n",
    "    'b1_1': tf.get_variable(\"b1_1\", shape = [64], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b1_2': tf.get_variable(\"b1_2\", shape = [64], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b1_3': tf.get_variable(\"b1_3\", shape = [64], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b1_4': tf.get_variable(\"b1_4\", shape = [64], initializer = tf.constant_initializer(value=0.0)),\n",
    "    \n",
    "    'b2_1': tf.get_variable(\"b2_1\", shape = [128], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b2_2': tf.get_variable(\"b2_2\", shape = [128], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b2_3': tf.get_variable(\"b2_3\", shape = [128], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b2_4': tf.get_variable(\"b2_4\", shape = [128], initializer = tf.constant_initializer(value=0.0)),\n",
    "    \n",
    "    'b3_1': tf.get_variable(\"b3_1\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b3_2': tf.get_variable(\"b3_2\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b3_3': tf.get_variable(\"b3_3\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b3_4': tf.get_variable(\"b3_4\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    \n",
    "    'b4_1': tf.get_variable(\"b4_1\", shape = [512], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b4_2': tf.get_variable(\"b4_2\", shape = [512], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b4_3': tf.get_variable(\"b4_3\", shape = [512], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b4_4': tf.get_variable(\"b4_4\", shape = [512], initializer = tf.constant_initializer(value=0.0)),\n",
    "    \n",
    "    'b5_1': tf.get_variable(\"b5_1\", shape = [1024], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b5_2': tf.get_variable(\"b5_2\", shape = [1024], initializer = tf.constant_initializer(value=0.0)),\n",
    "    \n",
    "    'b5': tf.get_variable(\"bd4\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b6': tf.get_variable(\"bd3\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'b7': tf.get_variable(\"bd2\", shape = [256], initializer = tf.constant_initializer(value=0.0)),\n",
    "    'bdp': tf.get_variable(\"bd1\", shape = [256], initializer = tf.constant_initializer(value=0.0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# U-net Model\n",
    "\n",
    "def Unpooling(inputOrg, size, mask=None):\n",
    "    m = size[0]\n",
    "    h = size[1]\n",
    "    w = size[2]\n",
    "    c = size[3]\n",
    "    input = tf.transpose(inputOrg, [0, 3, 1, 2])\n",
    "    x = tf.reshape(input, [-1, 1])\n",
    "    k = np.float32(np.array([1.0, 1.0]).reshape([1,-1]))\n",
    "    output = tf.matmul(x, k)\n",
    "    output = tf.reshape(output,[-1, c, h, w * 2])\n",
    "    xx = tf.transpose(output, [0, 1, 3, 2])\n",
    "    xx = tf.reshape(xx,[-1, 1])\n",
    "    output = tf.matmul(xx, k)\n",
    "    output = tf.reshape(output, [-1, c, w * 2, h * 2])\n",
    "    output = tf.transpose(output, [0, 3, 2, 1])\n",
    "    outshape = tf.stack([m, h * 2, w * 2, c])\n",
    "    if mask != None:\n",
    "        dense_mask = tf.sparse_to_dense(mask, outshape, output, 0)\n",
    "        return output, dense_mask\n",
    "    else:\n",
    "        return output\n",
    "    \n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "    \n",
    "    \n",
    "def Model(_X, _W, _b, _keepprob,is_training):\n",
    "\n",
    "    layer1 = tf.nn.conv2d(_X, _W['conv1_1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1, _b['b1_1'])\n",
    "           \n",
    "    layer1 = Batch_Normalization(layer1, training=is_training, scope=\"bn_layer1_1\")\n",
    "    \n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(layer1, _W['conv1_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1, _b['b1_2'])\n",
    "           \n",
    "    layer1 = Batch_Normalization(layer1, training=is_training, scope=\"bn_layer1_2\")\n",
    "    \n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    #########################################################################################\n",
    "    \n",
    "    layer2 = tf.nn.max_pool(layer1,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer2, _W['conv2_1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2, _b['b2_1'])\n",
    "           \n",
    "    layer2 = Batch_Normalization(layer2, training=is_training, scope=\"bn_layer2_1\")\n",
    "    \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer2, _W['conv2_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2, _b['b2_2'])\n",
    "           \n",
    "    layer2 = Batch_Normalization(layer2, training=is_training, scope=\"bn_layer2_2\")\n",
    "    \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer3 = tf.nn.max_pool(layer2,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer3, _W['conv3_1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3, _b['b3_1'])\n",
    "           \n",
    "    layer3 = Batch_Normalization(layer3, training=is_training, scope=\"bn_layer3_1\")\n",
    "    \n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer3, _W['conv3_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3, _b['b3_2'])\n",
    "           \n",
    "    layer3 = Batch_Normalization(layer3, training=is_training, scope=\"bn_layer3_2\")\n",
    "    \n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer4 = tf.nn.max_pool(layer3,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer4, _W['conv4_1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4, _b['b4_1'])\n",
    "           \n",
    "    layer4 = Batch_Normalization(layer4, training=is_training, scope=\"bn_layer4_1\")\n",
    "    \n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer4, _W['conv4_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4, _b['b4_2'])\n",
    "           \n",
    "    layer4 = Batch_Normalization(layer4, training=is_training, scope=\"bn_layer4_2\")\n",
    "    \n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer5 = tf.nn.max_pool(layer4,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    layer5 = tf.nn.conv2d(layer5, _W['conv5_1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer5 = tf.nn.bias_add(layer5, _b['b5_1'])\n",
    "           \n",
    "    layer5 = Batch_Normalization(layer5, training=is_training, scope=\"bn_layer5_1\")\n",
    "    \n",
    "    layer5 = tf.nn.relu(layer5)\n",
    "    \n",
    "    layer5 = tf.nn.conv2d(layer5, _W['conv5_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer5 = tf.nn.bias_add(layer5, _b['b5_2'])\n",
    "           \n",
    "    layer5 = Batch_Normalization(layer5, training=is_training, scope=\"bn_layer5_2\")\n",
    "    \n",
    "    layer5 = tf.nn.relu(layer5)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer5 = tf.nn.conv2d_transpose(layer5, _W['conv5_de'],tf.stack([tf.shape(_X)[0],int(height/8),int(width/8),512]) , [1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "    layer4 = tf.concat([layer4,layer5],axis=3)\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer4, _W['conv4_3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4, _b['b4_3'])\n",
    "           \n",
    "    layer4 = Batch_Normalization(layer5, training=is_training, scope=\"bn_layer4_3\")\n",
    "    \n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    \n",
    "    layer4 = tf.nn.conv2d(layer4, _W['conv4_4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4, _b['b4_4'])\n",
    "           \n",
    "    layer4 = Batch_Normalization(layer5, training=is_training, scope=\"bn_layer4_4\")\n",
    "    \n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer4 = tf.nn.conv2d_transpose(layer4, _W['conv4_de'],tf.stack([tf.shape(_X)[0],int(height/4),int(width/4),256]) , [1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "    layer3 = tf.concat([layer3,layer4],axis=3)\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer3, _W['conv3_3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3, _b['b3_3'])\n",
    "           \n",
    "    layer3 = Batch_Normalization(layer3, training=is_training, scope=\"bn_layer3_3\")\n",
    "    \n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    layer3 = tf.nn.conv2d(layer3, _W['conv3_4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer3 = tf.nn.bias_add(layer3, _b['b3_4'])\n",
    "           \n",
    "    layer3 = Batch_Normalization(layer3, training=is_training, scope=\"bn_layer3_4\")\n",
    "    \n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer3 = tf.nn.conv2d_transpose(layer3, _W['conv3_de'],tf.stack([tf.shape(_X)[0],int(height/2),int(width/2),128]) , [1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "    layer2 = tf.concat([layer2,layer3],axis=3)\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer2, _W['conv2_3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2, _b['b2_3'])\n",
    "           \n",
    "    layer2 = Batch_Normalization(layer2, training=is_training, scope=\"bn_layer2_3\")\n",
    "    \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    layer2 = tf.nn.conv2d(layer2, _W['conv2_4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer2 = tf.nn.bias_add(layer2, _b['b2_4'])\n",
    "           \n",
    "    layer2 = Batch_Normalization(layer2, training=is_training, scope=\"bn_layer2_4\")\n",
    "    \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    layer2 = tf.nn.conv2d_transpose(layer2, _W['conv2_de'],tf.stack([tf.shape(_X)[0],height,width,64]) , [1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "    layer1 = tf.concat([layer1,layer2],axis=3)\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(layer1, _W['conv1_3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1, _b['b1_3'])\n",
    "           \n",
    "    layer1 = Batch_Normalization(layer1, training=is_training, scope=\"bn_layer1_3\")\n",
    "    \n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(layer1, _W['conv1_4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer1 = tf.nn.bias_add(layer1, _b['b1_4'])\n",
    "           \n",
    "    layer1 = Batch_Normalization(layer1, training=is_training, scope=\"bn_layer1_4\")\n",
    "    \n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    \n",
    "    output = tf.nn.conv2d(layer1, _W['dense_inner_prod'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    return tf.reshape(output,[-1,height,width])\n",
    "\n",
    "pred = Model(x, weights, biases,p_keep_conv,is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-876ea576a4c7>:5: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/disc_segmentation_model/max.model\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "ckpt = tf.train.get_checkpoint_state('./model/disc_segmentation_model/')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop_margin_PIL(image):\n",
    "    im = image\n",
    "    np_img = np.asarray(im)\n",
    "    mean_pix = np.mean(np_img)\n",
    "    pix = im.load()\n",
    "    height, width = im.size\n",
    "\n",
    "    c_x, c_y = (int(height / 2), int(width / 2))\n",
    "\n",
    "    for y in range(c_y):\n",
    "        if sum(pix[c_x, y]) > mean_pix:\n",
    "            left = (c_x, y)\n",
    "            break;\n",
    "\n",
    "    for x in range(c_x):\n",
    "        if sum(pix[x, c_y]) > mean_pix:\n",
    "            up = (x, c_y)\n",
    "            break;\n",
    "\n",
    "    crop_img = im.crop((up[0], left[1], left[0], up[1]))\n",
    "\n",
    "    diameter_height = up[1] - left[1]\n",
    "    diameter_width = left[0] - up[0]\n",
    "\n",
    "    crop_img = im.crop((up[0], left[1], left[0] + diameter_width, up[1] + diameter_height))\n",
    "\n",
    "    return crop_img\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\"\n",
    "\n",
    "img_dir = \"/home/mediwhale/SEED/SEED_1/RETINA - SCES/SICC05/2009-02-09/CS50023/CS50023_KK00(POZ.002.JPG\"\n",
    "\n",
    "img_origin = Image.open(img_dir)\n",
    "img_origin = crop_margin_PIL(img_origin)\n",
    "img = img_origin.convert(\"L\").resize((256,256))\n",
    "img_arr = np.array(img)\n",
    "\n",
    "img_arr = img_arr.reshape(1,height,width,1)\n",
    "img_arr = img_arr/255.\n",
    "\n",
    "p = sess.run([pred], feed_dict={x: img_arr,p_keep_conv:1.0,is_training:False})\n",
    "p = np.array(p)\n",
    "p = sigmoid(p)\n",
    "p = p.reshape(height,width)\n",
    "\n",
    "threshold = p.max() - 0.1\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (p > 0.2).sum() < 50 :\n",
    "    result = \"other\"\n",
    "else :\n",
    "    top = (p[0:95,0:85] > threshold).sum() + (p[0:60,85:170] > threshold).sum() + (p[0:95,170:] > threshold).sum()\n",
    "    center = (p[95:160,0:85] > threshold).sum() + (p[60:196,85:170] > threshold).sum() + (p[95:160,170:] > threshold).sum()\n",
    "    bottom = (p[160:,0:85] > threshold).sum() + (p[196:,85:170] > threshold).sum() + (p[160:,170:] > threshold).sum()\n",
    "    \n",
    "    if top > center or bottom > center :\n",
    "        result = \"other\"\n",
    "    else :\n",
    "        left = (p[80:176,:85]>threshold).sum()\n",
    "        center = (p[80:176,85:170]>threshold).sum()\n",
    "        right = (p[80:176,170:]>threshold).sum()\n",
    "        \n",
    "        l = np.argmax([left,center,right])\n",
    "        \n",
    "        if l == 0 :\n",
    "            result = \"macular_focus_left\"\n",
    "        elif l == 1 :\n",
    "            result = \"disc_focus\"\n",
    "        elif l == 2 :\n",
    "            result = \"macular_focus_right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_focus\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
